{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd28631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15d2c0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=5, bias=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88222313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- utils ----------\n",
    "def mlp(sizes, act=nn.ReLU, out_act=None):\n",
    "    layers = []\n",
    "    for i in range(len(sizes)-1):\n",
    "        layers += [nn.Linear(sizes[i], sizes[i+1])]\n",
    "        if i < len(sizes)-2:\n",
    "            layers += [act()]\n",
    "        elif out_act is not None:\n",
    "            layers += [out_act()]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e277639b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "output = mlp([3, 64, 64, 1], act=nn.ReLU, out_act=None)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30f41561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3, 4]\n",
    "print(x[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47d0afcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4])\n",
    "torch.unsqueeze(x, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef76617b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4])\n",
    "torch.unsqueeze(x, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- relational energy E_theta ----------\n",
    "class RelationalEnergy(nn.Module):\n",
    "    \"\"\"\n",
    "    E_theta(x, a, w) = [ f_theta( sum_{t,i,j} sig(a_i)*sig(a_j) * g_theta(x_t_i, x_t_j, w), w ) ]^2\n",
    "    x: [B, T, N, Dx]                                        ## Batch, Time, Num entities, Dim entity\n",
    "    a: [B, N]    (real-valued, gated via sigmoid inside) \n",
    "    w: [B, Dw]                                              ## Batch, Dim concept code\n",
    "    returns scalar energy [B]\n",
    "    \"\"\"\n",
    "    def __init__(self, Dx, Dw, hidden=128, T_weighted=True):\n",
    "        super().__init__()\n",
    "        self.Dx, self.Dw = Dx, Dw\n",
    "        # g takes concatenated (x_i, x_j, w)\n",
    "        self.g = mlp([2*Dx + Dw, hidden, hidden, hidden])\n",
    "        # f takes pooled sum over pairs (+ w)\n",
    "        self.f = mlp([hidden + Dw, hidden, hidden, 1])  # scalar before square\n",
    "        self.T_weighted = T_weighted\n",
    "\n",
    "    def forward(self, x, a, w):\n",
    "        B, T, N, Dx = x.shape\n",
    "        sig_a = torch.sigmoid(a)                          # [B, N]\n",
    "        # prepare pairwise masks m_ij = sig(a_i) * sig(a_j)\n",
    "        m = sig_a.unsqueeze(2) * sig_a.unsqueeze(1)       # [B, N, N]\n",
    "        # expand across time\n",
    "        m = m.unsqueeze(1).expand(B, T, N, N)             # [B, T, N, N]\n",
    "\n",
    "        # make pairwise feature tensor: [B,T,N,N, 2*Dx + Dw]\n",
    "        xi = x.unsqueeze(3).expand(B, T, N, N, Dx)\n",
    "        xj = x.unsqueeze(2).expand(B, T, N, N, Dx)\n",
    "        w_exp = w.view(B,1,1,1,-1).expand(B,T,N,N,-1)\n",
    "        pair_feat = torch.cat([xi, xj, w_exp], dim=-1)    # [..., 2Dx+Dw]\n",
    "\n",
    "        # apply g to each pair\n",
    "        g_ij = self.g(pair_feat)                          # [B,T,N,N,H]\n",
    "        # gate by m\n",
    "        g_ij = g_ij * m.unsqueeze(-1)                     # [B,T,N,N,H]\n",
    "        # sum over pairs and time\n",
    "        pooled = g_ij.sum(dim=(1,2,3))                    # [B,H]\n",
    "\n",
    "        # fuse with w and score\n",
    "        f_in = torch.cat([pooled, w], dim=-1)             # [B,H+Dw]\n",
    "        out = self.f(f_in).squeeze(-1)                    # [B]\n",
    "        energy = out.pow(2)                               # non-negative\n",
    "        return energy\n",
    "\n",
    "# ---------- SGLD samplers ----------\n",
    "def sgld_step(var, grad, alpha):\n",
    "    noise = torch.randn_like(var) * (alpha ** 0.5)\n",
    "    return var + 0.5 * alpha * grad + noise\n",
    "\n",
    "@torch.no_grad()\n",
    "def sgld_optimize_x(E, x_init, a, w, steps=10, alpha=1e-2):\n",
    "    x = x_init.clone().requires_grad_(True)\n",
    "    for _ in range(steps):\n",
    "        E_x = E(x, a, w).sum()\n",
    "        grad, = torch.autograd.grad(E_x, x, create_graph=False)\n",
    "        x = sgld_step(x, grad, alpha).detach().requires_grad_(True)\n",
    "    return x.detach()\n",
    "\n",
    "@torch.no_grad()\n",
    "def sgld_optimize_a(E, x, a_init, w, steps=10, alpha=1e-2):\n",
    "    a = a_init.clone().requires_grad_(True)\n",
    "    for _ in range(steps):\n",
    "        E_a = E(x, a, w).sum()\n",
    "        grad, = torch.autograd.grad(E_a, a, create_graph=False)\n",
    "        a = sgld_step(a, grad, alpha).detach().requires_grad_(True)\n",
    "    return a.detach()\n",
    "\n",
    "# ---------- execution-time concept inference (inner loop) ----------\n",
    "def infer_concept_codes(E, demos, Dw, steps=10, lr=0.1):\n",
    "    \"\"\"\n",
    "    demos: dict with tensors from X_demo:\n",
    "      x0_demo: [B,T,N,Dx], x1_demo: [B,T,N,Dx], a_demo: [B,N]\n",
    "    Returns: w_x, w_a each [B, Dw]\n",
    "    \"\"\"\n",
    "    B = demos['x0'].shape[0]\n",
    "    w_x = torch.randn(B, Dw, device=demos['x0'].device, requires_grad=True)\n",
    "    w_a = torch.randn(B, Dw, device=demos['x0'].device, requires_grad=True)\n",
    "    opt = torch.optim.SGD([w_x, w_a], lr=lr)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        opt.zero_grad()\n",
    "        Ex = E(demos['x1'], demos['a'], w_x)   # generation consistency\n",
    "        Ea = E(demos['x0'], demos['a'], w_a)   # identification consistency\n",
    "        loss = (Ex + Ea).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return w_x.detach(), w_a.detach()\n",
    "\n",
    "# ---------- training step (outer loop) ----------\n",
    "def training_step(E, batch_demo, batch_train, opt_theta, K=10, alpha=1e-2, lam=1.0):\n",
    "    \"\"\"\n",
    "    batch_demo / batch_train contain x0, x1, a (same shapes)\n",
    "    \"\"\"\n",
    "    # 1) infer concept codes from demos (stop-grad w.r.t theta, per paperâ€™s simplification)\n",
    "    with torch.no_grad():\n",
    "        w_x, w_a = infer_concept_codes(E, batch_demo, Dw=E.Dw if hasattr(E, 'Dw') else E.f[0].in_features, steps=K)\n",
    "\n",
    "    # 2) sample negatives via SGLD\n",
    "    x0, x1, a = batch_train['x0'], batch_train['x1'], batch_train['a']\n",
    "    a_init = torch.randn_like(a)\n",
    "    x_tilde = sgld_optimize_x(E, x0, a, w_x, steps=K, alpha=alpha)\n",
    "    a_tilde = sgld_optimize_a(E, x0, a_init, w_a, steps=K, alpha=alpha)\n",
    "\n",
    "    # 3) compute losses (contrastive + KL)\n",
    "    Ex_pos = E(x1, a, w_x)\n",
    "    Ex_neg = E(x_tilde, a, w_x)\n",
    "    La_pos = E(x0, a, w_a)\n",
    "    La_neg = E(x0, a_tilde, w_a)\n",
    "\n",
    "    Lx = F.softplus(Ex_pos - Ex_neg).mean()\n",
    "    La = F.softplus(La_pos - La_neg).mean()\n",
    "    L_ml = Lx + La\n",
    "\n",
    "    L_kl = (Ex_neg + La_neg).mean()  # entropy terms omitted (constant with fixed noise)\n",
    "\n",
    "    loss = L_ml + lam * L_kl\n",
    "\n",
    "    opt_theta.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(E.parameters(), 1.0)\n",
    "    opt_theta.step()\n",
    "\n",
    "    return {\n",
    "        'loss': loss.item(),\n",
    "        'L_ml': L_ml.item(),\n",
    "        'L_kl': L_kl.item(),\n",
    "        'Ex_pos': Ex_pos.mean().item(),\n",
    "        'Ex_neg': Ex_neg.mean().item()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_line_concept(B, T, N, Dx=2, noise=0.01, length=1.0, device='cpu'):\n",
    "    # x0: random positions in [-1,1]^2\n",
    "    x0 = torch.rand(B, T, N, Dx, device=device)*2-1\n",
    "    # choose k attended entities (e.g., k=4)\n",
    "    k = 4\n",
    "    a = torch.zeros(B, N, device=device)\n",
    "    for b in range(B):\n",
    "        idx = torch.randperm(N)[:k]\n",
    "        a[b, idx] = 3.0   # unbounded; model will pass through sigmoid\n",
    "        # make those k near a random line at t=1\n",
    "        p0 = torch.rand(2, device=device)*2-1\n",
    "        direction = F.normalize(torch.rand(2, device=device)*2-1, dim=0)\n",
    "        t_vals = torch.linspace(-length/2, length/2, k, device=device)\n",
    "        line_pts = p0 + t_vals[:,None]*direction\n",
    "        x0[b, -1, idx, :2] = line_pts + noise*torch.randn_like(line_pts)\n",
    "    # make x1 = last frame; you can interpolate or set only t=1\n",
    "    x1 = x0.clone()\n",
    "    return {'x0': x0, 'x1': x1, 'a': a}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperenv",
   "language": "python",
   "name": "simon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
