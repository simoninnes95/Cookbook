df.na.drop(subset=["col_X"])

df.filter(df.col_X.isNotNull())

# cast types
from pyspark.sql.types import IntegerType

df = df.withColumn("pipe_width", F.col("pipe_width").cast(IntegerType()))

from pyspark.sql.functions import unix_timestamp, from_unixtime

df = spark.createDataFrame(
    [("11/25/1991",), ("11/24/1991",), ("11/30/1991",)], 
    ['date_str']
)

df2 = df.select(
    'date_str', 
    from_unixtime(unix_timestamp('date_str', 'MM/dd/yyy')).alias('date')
)

# Using spark functions timestamp

from pyspark.sql.functions import col, unix_timestamp, to_date

#sample data
df = sc.parallelize([['12-21-2006'],
                     ['05-30-2007'],
                     ['01-01-1984'],
                     ['12-24-2017']]).toDF(["date_in_strFormat"])
df.printSchema()

df = df.withColumn('date_in_dateFormat', 
                   to_date(unix_timestamp(col('date_in_strFormat'), 'MM-dd-yyyy').cast("timestamp")))
df.show()
df.printSchema()

# count distinct

df.select(F.countDistinct("colName")).show()

# max or min

df.select(F.min("DateTime")).show()

# filter based on list

not in list
df.filter(~F.col("SiteID").isin(list))

# create dataframe

columns = ["site_id","date_time", "flow"]
data = [
          ("logger1", "2021/01/22", "200"), ("logger2", "2021/01/23", "150"), ("logger1", "2021/01/24", "320"),
          ("Logger1", "2021/01/25", "240"), ("Logger1", "2021/01/27", "100"), ("LOGGER2", "2021/01/28", "160"),
          ("LOGGER1", "2021/01/29", "560"), ("logger1", "2021/01/30", "180"), ("logger1", "2021/02/01", "410"),
       ]

rdd = spark.sparkContext.parallelize(data)

df_from_rdd = rdd.toDF(columns)
df_from_rdd.printSchema()

# pandas udf

@F.pandas_udf(schema, F.PandasUDFType.GROUPED_MAP)
def impute(data: DataFrame) -> DataFrame:
  print("YYY")
  """Imputes missing values using backfill method of TimestampStart and TimestampEnd columns
  
  Params
  ======
  pdf: Pandas DataFrame
  
  Returns
  =======
  df: sql DataFrame
  """
  n, p = data.shape
  data = data.sort_values("DateCopied")
  data['TimestampStart'] = data['TimestampStart'].fillna(method='bfill', limit=n)
  data['TimestampEnd'] = data['TimestampEnd'].fillna(method='bfill', limit=n)
  print(data.columns)
  return data

missing_data = site_levels.groupBy("SiteName").apply(impute)

# broadcast

from pyspark.context import SparkContext

sc = SparkContext('local', 'test')

b = sc.broadcast([1, 2, 3, 4, 5])

b.value
[1, 2, 3, 4, 5]

sc.parallelize([0, 0]).flatMap(lambda x: b.value).collect()
[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]

b.unpersist()

# joins using window() functions

first approach

df_agg = df.groupBy('city', 'team').agg(F.mean('job').alias('job_mean'))

df = df.join(df_agg, on=['city', 'team'], how='inner')

second approach

from pyspark.sql.window import Window

window_spec = Window.partitionBy(df['city'], df['team'])
df = df.withColumn('job_mean', F.mean(col('job')).over(window_spec))

#Optimisation

Zorder

F.broadcast

Checkpointing
