df.na.drop(subset=["col_X"])

df.filter(df.col_X.isNotNull())

# cast types
from pyspark.sql.types import IntegerType

df = df.withColumn("pipe_width", F.col("pipe_width").cast(IntegerType()))

from pyspark.sql.functions import unix_timestamp, from_unixtime

df = spark.createDataFrame(
    [("11/25/1991",), ("11/24/1991",), ("11/30/1991",)], 
    ['date_str']
)

df2 = df.select(
    'date_str', 
    from_unixtime(unix_timestamp('date_str', 'MM/dd/yyy')).alias('date')
)

# Using spark functions timestamp

from pyspark.sql.functions import col, unix_timestamp, to_date

#sample data
df = sc.parallelize([['12-21-2006'],
                     ['05-30-2007'],
                     ['01-01-1984'],
                     ['12-24-2017']]).toDF(["date_in_strFormat"])
df.printSchema()

df = df.withColumn('date_in_dateFormat', 
                   to_date(unix_timestamp(col('date_in_strFormat'), 'MM-dd-yyyy').cast("timestamp")))
df.show()
df.printSchema()

# count distinct

df.select(F.countDistinct("colName")).show()

# max or min

df.select(F.min("DateTime")).show()

# filter based on list

not in list
df.filter(~F.col("SiteID").isin(list))